{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c45a738-99e2-4b39-9733-748dc706a734",
   "metadata": {},
   "source": [
    "# Vendor Performance Analysis: Data Ingestion and Setup\n",
    "\n",
    "This notebook is the first step in a larger project focused on analyzing vendor performance. Its primary purpose is to efficiently ingest raw data from several large CSV files into a centralized SQLite database.\n",
    "\n",
    "By using a chunking strategy, this script can handle multi-gigabyte files like `sales.csv` without running into memory issues, ensuring the data is ready for subsequent analysis, transformation, and dashboard creation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3632c04-2349-4d24-95b8-35b9f192b430",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n",
    "First, we'll import the necessary libraries and establish a connection to our database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "14db4669-7d59-4f54-a12e-9b19eba36723",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09a6ad3a-e3a2-4374-b334-1467dbf80b1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".ipynb_checkpoints\n",
      "begin_inventory.csv\n",
      "end_inventory.csv\n",
      "purchases.csv\n",
      "purchase_prices.csv\n",
      "sales.csv\n",
      "vendor_invoice.csv\n"
     ]
    }
   ],
   "source": [
    "engine = create_engine('sqlite:///inventory.db')\n",
    "for file in os.listdir('data'):\n",
    "    print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ded251ae-952f-4004-a9a4-284a8fd759ee",
   "metadata": {},
   "source": [
    "## 2. Data Exploration\n",
    "\n",
    "Before ingesting the data, it's a good practice to understand its structure and size. This step helps us identify which files might be too large to load directly into memory, justifying our use of chunking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c580f93f-df59-4418-ac52-faa34418bced",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "begin_inventory.csv: (206529, 9)\n",
      "end_inventory.csv: (224489, 9)\n",
      "purchases.csv: (2372474, 16)\n",
      "purchase_prices.csv: (12261, 9)\n",
      "sales.csv: (12825363, 14)\n",
      "vendor_invoice.csv: (5543, 10)\n"
     ]
    }
   ],
   "source": [
    "# Iterate through files in 'data' directory\n",
    "for file in os.listdir('data'):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join('data', file)\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"{file}: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a4dec8e-0ee0-4835-94e4-b5c116cb4839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing begin_inventory.csv (16.64 MB)...\n",
      "Processing end_inventory.csv (18.10 MB)...\n",
      "Processing purchases.csv (344.83 MB)...\n",
      "Processing purchase_prices.csv (1.00 MB)...\n",
      "Processing sales.csv (1522.76 MB)...\n",
      "Processing vendor_invoice.csv (0.49 MB)...\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('data'):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join('data', file)\n",
    "        size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "        print(f\"Processing {file} ({size_mb:.2f} MB)...\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65845065-265b-4575-91bf-81919f545148",
   "metadata": {},
   "source": [
    "## 3. Chunked Data Ingestion Function\n",
    "\n",
    "The core of this notebook is a function designed for efficient data ingestion. This method reads large files in smaller, manageable chunks, which is essential for working with big datasets without exhausting system memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac0f649e-ea40-4d5d-ae75-bba4d8453a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_csv_in_chunks(file_path, table_name, engine, chunk_size=100000):\n",
    "    \"\"\"\n",
    "    Ingests a large CSV file into a database in chunks.\n",
    "\n",
    "    Parameters:\n",
    "        file_path (str): Path to the CSV file.\n",
    "        table_name (str): Name of the target table.\n",
    "        engine (SQLAlchemy Engine): SQLAlchemy database engine.\n",
    "        chunk_size (int): Number of rows per chunk.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    first_chunk = True\n",
    "    for chunk in pd.read_csv(file_path, chunksize=chunk_size):\n",
    "        if_exists_mode = 'replace' if first_chunk else 'append'\n",
    "        chunk.to_sql(table_name, con=engine, if_exists=if_exists_mode, index=False)\n",
    "        first_chunk = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "869ae31c-c807-41ec-851e-da094e645fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunked ingesting: begin_inventory.csv\n",
      "Chunked ingesting: end_inventory.csv\n",
      "Chunked ingesting: purchases.csv\n",
      "Chunked ingesting: purchase_prices.csv\n",
      "Chunked ingesting: sales.csv\n",
      "Chunked ingesting: vendor_invoice.csv\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('data'):\n",
    "    if file.endswith('.csv'):\n",
    "        file_path = os.path.join('data', file)\n",
    "        table_name = os.path.splitext(file)[0]\n",
    "        print(f\"Chunked ingesting: {file}\")\n",
    "        ingest_csv_in_chunks(file_path, table_name, engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974185a0-d71a-45ed-b705-04629b422f62",
   "metadata": {},
   "source": [
    "## 4. Main Ingestion Pipeline\n",
    "\n",
    "To create a robust and reproducible process, we'll wrap the ingestion logic in a single function. This function includes logging and error handling, providing a detailed record of the ingestion process and ensuring that any potential issues are captured."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "148c3dc0-5693-466b-956f-94aaca27c5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "\n",
    "# Ensure the logs directory exists\n",
    "os.makedirs(\"logs\", exist_ok=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename=\"logs/ingestion_db.log\",\n",
    "    level=logging.DEBUG,  # DEBUG captures everything\n",
    "    format=\"%(asctime)s - %(levelname)s - %(message)s\",\n",
    "    filemode=\"a\"# Append mode; use \"w\" to overwrite each time\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c21a97f-a52a-4c0a-b89c-4ff6cd69dfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def load_raw_data():\n",
    "    '''Load CSVs from data/ and ingest into DB'''\n",
    "    data_dir = 'data'\n",
    "    \n",
    "    if not os.path.exists(data_dir):\n",
    "        logging.error(f\"Data directory '{data_dir}' does not exist.\")\n",
    "        return\n",
    "    \n",
    "    logging.info(\"Starting to load raw CSV files from 'data' directory.\")\n",
    "\n",
    "    start = time.time()\n",
    "    \n",
    "    for file in os.listdir(data_dir):\n",
    "        if file.endswith('.csv'):\n",
    "            file_path = os.path.join(data_dir, file)\n",
    "            table_name = os.path.splitext(file)[0]\n",
    "            \n",
    "            logging.info(f\"Starting chunked ingestion for file: {file}\")\n",
    "            \n",
    "            try:\n",
    "                ingest_csv_in_chunks(file_path, table_name, engine)\n",
    "                logging.info(f\"Successfully ingested file: {file} into table: {table_name}\")\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Failed to ingest file: {file} â€” Error: {e}\")\n",
    "    \n",
    "    end = time.time()\n",
    "    total_time = (end - start) / 60  # in minutes\n",
    "    logging.info(\"Finished loading all raw CSV files.\")\n",
    "    logging.info(f\"Total Time Taken: {total_time:.2f} minutes\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    load_raw_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc33fc4b-8189-4393-80fb-e361af220acf",
   "metadata": {},
   "source": [
    "## 5. Conclusion and Next Steps\n",
    "\n",
    "All raw data from the CSV files has now been successfully ingested and stored in the `inventory.db` SQLite database.\n",
    "\n",
    "**Next Steps:**\n",
    "1.  **Data Cleaning:** Inspect and clean the data within the database tables.\n",
    "2.  **Data Transformation:** Create new tables or views to prepare the data for analysis (e.g., aggregating sales by vendor).\n",
    "3.  **Analysis and Dashboarding:** Use this clean data to perform a vendor performance analysis and build a dashboard for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cb74fd-899d-424f-b6ed-48442b947cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
